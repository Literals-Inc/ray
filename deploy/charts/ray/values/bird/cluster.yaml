# operatorOnly - If true, will only set up the Operator with this release, without launching a Ray cluster.
operatorOnly: false
clusterOnly: true

code_bucket: glass-development-ci
code_bucket_path: dev
ray_version: 1.6.0

# image is Ray image to use for the head and workers of this Ray cluster.
image: us-west1-docker.pkg.dev/glass-development-darkhelmet/ray/ray-node:{{ .Values.ray_version }}

# headPodType is the podType used for the Ray head node (as configured below).
headPodType: rayHeadType

# podTypes is the list of pod configurations available for use as Ray nodes.
podTypes:
  # The key for each podType is a user-defined string.
  # Since we set headPodType: rayHeadType, the Ray head pod will use the configuration
  # defined in this entry of podTypes:
  rayHeadType:
    # No worker pods of this pod type (just the head). Thus, we set minWorkers and maxWorkers to 0.
    minWorkers: 0
    maxWorkers: 0
    # CPU is the number of CPUs used by this pod type.
    # (Used for both requests and limits. Must be an integer, as Ray does not support fractional CPUs.)
    CPU: 1
    # memory is the memory used by this Pod type.
    # (Used for both requests and limits.)
    memory: 5Gi
    # GPU is the number of NVIDIA GPUs used by this pod type.
    # (Optional, requires GPU nodes with appropriate setup. See https://docs.ray.io/en/master/cluster/kubernetes-gpu.html)
    GPU: 0
    # rayResources is an optional string-int mapping signalling additional resources to Ray.
    # "CPU", "GPU", and "memory" are filled automatically based on the above settings, but can be overriden;
    # For example, rayResources: {"CPU": 0} can be used in the head podType to prevent Ray from scheduling tasks on the head.
    # See https://docs.ray.io/en/master/advanced.html#dynamic-remote-parameters for an example of usage of custom resources in a Ray task.
    rayResources: {"CPU": 0}
    # Optionally, set a node selector for this podType: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      literal.ai/node-type: ray-node
      cloud.google.com/gke-nodepool: ray-cpu
    serviceAccountName: ray-operator-serviceaccount
    env:
      - name: PYTHONPATH
        value: /root/ray/glass-video
      - name: RAY_WORKER_BASE_TMP_PATH
        value: /root/tmp/

  # The key for each podType is a user-defined string.
  rayWorkerType:
    # minWorkers is the minimum number of Ray workers of this pod type to keep running.
    minWorkers: 1
    # maxWorkers is the maximum number of Ray workers of this pod type to which Ray will scale.
    maxWorkers: 1
    # memory is the memory used by this Pod type.
    # (Used for both requests and limits.)
    memory: 25Gi
    # CPU is the number of CPUs used by this pod type.
    # (Used for both requests and limits. Must be an integer, as Ray does not support fractional CPUs.)
    CPU: 6
    # GPU is the number of NVIDIA GPUs used by this pod type.
    # (Optional, requires GPU nodes with appropriate setup. See https://docs.ray.io/en/master/cluster/kubernetes-gpu.html)
    GPU: 1
    # rayResources is an optional string-int mapping signalling additional resources to Ray.
    # "CPU", "GPU", and "memory" are filled automatically based on the above settings, but can be overriden;
    # For example, rayResources: {"CPU": 0} can be used in the head podType to prevent Ray from scheduling tasks on the head.
    # See https://docs.ray.io/en/master/advanced.html#dynamic-remote-parameters for an example of usage of custom resources in a Ray task.
    rayResources: {}
    # Optionally, set a node selector for this Pod type. See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      literal.ai/node-type: ray-node
      cloud.google.com/gke-nodepool: ray-gpu-big
    serviceAccountName: ray-operator-serviceaccount
    workerSetupCommands:
      - mkdir -p /root/ray
      - gsutil -m rsync -r gs://{{ .Values.code_bucket }}/ray/{{ .Values.code_bucket_path }}/ /root/ray/
      - cd /root/ray/glass-video && bash ./bin/ray_setup.sh
    env:
      - name: PYTHONPATH
        value: /root/ray/glass-video
      - name: RAY_WORKER_BASE_TMP_PATH
        value: /root/tmp/
